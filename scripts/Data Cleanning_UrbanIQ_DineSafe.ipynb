{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae9d523-abba-4100-a9a0-37c99f1bce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "380e225b-97a5-41a1-abcb-d6eb382bbc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deltalake\n",
      "  Downloading deltalake-1.2.1-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting arro3-core>=0.5.0 (from deltalake)\n",
      "  Downloading arro3_core-0.6.5-cp311-abi3-win_amd64.whl.metadata (363 bytes)\n",
      "Collecting deprecated>=1.2.18 (from deltalake)\n",
      "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from deprecated>=1.2.18->deltalake) (1.14.1)\n",
      "Downloading deltalake-1.2.1-cp39-abi3-win_amd64.whl (47.9 MB)\n",
      "   ---------------------------------------- 0.0/47.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 11.0/47.9 MB 76.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 33.0/47.9 MB 91.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  47.7/47.9 MB 98.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  47.7/47.9 MB 98.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 47.9/47.9 MB 52.6 MB/s eta 0:00:00\n",
      "Downloading arro3_core-0.6.5-cp311-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 80.9 MB/s eta 0:00:00\n",
      "Downloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: deprecated, arro3-core, deltalake\n",
      "Successfully installed arro3-core-0.6.5 deltalake-1.2.1 deprecated-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install deltalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40cb087f-ff47-4eda-898e-cca7860175c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found existing local file. Skipping download.\n",
      "\n",
      "üìÇ Loading dataset...\n",
      "‚úÖ Dataset loaded: 124740 rows, 17 columns\n",
      "\n",
      "üîç Preview of raw data:\n",
      "   _id  Establishment ID  Inspection ID          Establishment Name  \\\n",
      "0    1          10657713    105133203.0       NEW KANTAMANTO MARKET   \n",
      "1    2          10657713    105133203.0       NEW KANTAMANTO MARKET   \n",
      "2    3          10657713    105238109.0       NEW KANTAMANTO MARKET   \n",
      "3   10          10752656    105149282.0  # HASHTAG INDIA RESTAURANT   \n",
      "4   11          10752656    105149282.0  # HASHTAG INDIA RESTAURANT   \n",
      "\n",
      "  Establishment Type      Establishment Address Establishment Status  \\\n",
      "0         Food Depot  266 EDDYSTONE AVE, Unit-0                 Pass   \n",
      "1         Food Depot  266 EDDYSTONE AVE, Unit-0                 Pass   \n",
      "2         Food Depot  266 EDDYSTONE AVE, Unit-0                 Pass   \n",
      "3      Food Take Out           1871 O'CONNOR DR                 Pass   \n",
      "4      Food Take Out           1871 O'CONNOR DR                 Pass   \n",
      "\n",
      "  Min. Inspections Per Year  \\\n",
      "0                         2   \n",
      "1                         2   \n",
      "2                         2   \n",
      "3                         2   \n",
      "4                         2   \n",
      "\n",
      "                                  Infraction Details Inspection Date  \\\n",
      "0  FOOD PREMISE NOT MAINTAINED WITH CLEAN FLOORS ...        3/7/2023   \n",
      "1  Operate food premise - equipment not arranged ...        3/7/2023   \n",
      "2                                                NaN       8/25/2023   \n",
      "3  Food premise maintained in manner to permit co...        4/4/2023   \n",
      "4  USE FOOD EQUIPMENT NOT OF SOUND AND TIGHT CONS...        4/4/2023   \n",
      "\n",
      "          Severity            Action Outcome  Amount Fined  Latitude  \\\n",
      "0        M - Minor  Notice to Comply     NaN           NaN  43.74791   \n",
      "1        M - Minor  Notice to Comply     NaN           NaN  43.74791   \n",
      "2              NaN               NaN     NaN           NaN  43.74791   \n",
      "3      C - Crucial  Notice to Comply     NaN           NaN  43.72199   \n",
      "4  S - Significant  Notice to Comply     NaN           NaN  43.72199   \n",
      "\n",
      "   Longitude                         unique_id  \n",
      "0  -79.52219  6e10cefe79756f0320205ba4eed824b0  \n",
      "1  -79.52219  ed41cbcc89db93061c463f66bf8a98cc  \n",
      "2  -79.52219  5d3e7e93d5968017337246a9ee547631  \n",
      "3  -79.30349  46d953290f2ed5e41dba8b39ca211905  \n",
      "4  -79.30349  eb7b380e83d7b84cecde63456010cd66   \n",
      "\n",
      "Missing values per column:\n",
      "_id                               0\n",
      "Establishment ID                  0\n",
      "Inspection ID                  2898\n",
      "Establishment Name                0\n",
      "Establishment Type                0\n",
      "Establishment Address             0\n",
      "Establishment Status              0\n",
      "Min. Inspections Per Year         0\n",
      "Infraction Details            47141\n",
      "Inspection Date                2898\n",
      "Severity                      47141\n",
      "Action                        47141\n",
      "Outcome                      124269\n",
      "Amount Fined                 124436\n",
      "Latitude                          0\n",
      "Longitude                         0\n",
      "unique_id                         0\n",
      "dtype: int64 \n",
      "\n",
      "üîß Handling missing values...\n",
      "‚úÖ Missing values handled.\n",
      "\n",
      "_id                          0\n",
      "Establishment ID             0\n",
      "Inspection ID                0\n",
      "Establishment Name           0\n",
      "Establishment Type           0\n",
      "Establishment Address        0\n",
      "Establishment Status         0\n",
      "Min. Inspections Per Year    0\n",
      "Infraction Details           0\n",
      "Inspection Date              0\n",
      "Severity                     0\n",
      "Action                       0\n",
      "Outcome                      0\n",
      "Amount Fined                 0\n",
      "Latitude                     0\n",
      "Longitude                    0\n",
      "unique_id                    0\n",
      "dtype: int64\n",
      "üîç Found 0 duplicate rows before removal.\n",
      "‚úÖ Removed 0 duplicate rows.\n",
      "\n",
      "‚úÖ Data types standardized.\n",
      "\n",
      "‚úÖ Text fields cleaned.\n",
      "\n",
      "üíæ Cleaned dataset saved to: data_cleaned\\DineSafe_Cleaned.csv\n",
      "‚úÖ Final shape: 121842 rows, 17 columns\n",
      "üíæ Parquet file saved to: data_cleaned\\DineSafe_Cleaned.parquet\n",
      "üíæ Delta Lake versioned dataset created at: data_cleaned\\DineSafe_Delta\n",
      "‚úÖ Final shape: 121842 rows √ó 17 columns\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 1: Import Libraries ----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import os\n",
    "from deltalake import write_deltalake\n",
    "\n",
    "# ---------- Step 2: Download Raw Dataset from Google Drive ----------\n",
    "# File is stored in Google Drive (public share link)\n",
    "file_id = \"11OcLKGiakMl2cP3Dz4gVh31T2R01enPk\"\n",
    "raw_path = \"DineSafe.csv\"\n",
    "\n",
    "# Download the dataset if not already present\n",
    "if not os.path.exists(raw_path):\n",
    "    print(\"‚¨áÔ∏è Downloading DineSafe dataset from Google Drive...\")\n",
    "    gdown.download(id=file_id, output=raw_path, quiet=False)\n",
    "else:\n",
    "    print(\"‚úÖ Found existing local file. Skipping download.\")\n",
    "\n",
    "# ---------- Step 3: Load Dataset ----------\n",
    "print(\"\\nüìÇ Loading dataset...\")\n",
    "df = pd.read_csv(raw_path)\n",
    "print(f\"‚úÖ Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
    "\n",
    "# ---------- Step 4: Initial Overview ----------\n",
    "print(\"üîç Preview of raw data:\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# ---------- Step 5: Handle Missing Values ----------\n",
    "print(\"üîß Handling missing values...\")\n",
    "\n",
    "# Delete records with missing key fields\n",
    "df = df.dropna(subset=[\"Inspection ID\", \"Inspection Date\", \"Establishment Name\"])\n",
    "\n",
    "#  Fill in the missing logically\n",
    "df[\"Infraction Details\"] = df[\"Infraction Details\"].fillna(\"No Infraction\")\n",
    "df[\"Severity\"] = df[\"Severity\"].fillna(\"No Infraction\")\n",
    "df[\"Action\"] = df[\"Action\"].fillna(\"None\")\n",
    "df[\"Outcome\"] = df[\"Outcome\"].fillna(\"Pending/No Action\")\n",
    "df[\"Amount Fined\"] = df[\"Amount Fined\"].fillna(0)\n",
    "\n",
    "print(\"‚úÖ Missing values handled.\\n\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# ---------- Step 6: Remove Duplicates ----------\n",
    "\n",
    "dup_count = df.duplicated().sum()\n",
    "print(f\"üîç Found {dup_count} duplicate rows before removal.\")\n",
    "\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "after = df.shape[0]\n",
    "\n",
    "print(f\"‚úÖ Removed {before - after} duplicate rows.\\n\")\n",
    "\n",
    "\n",
    "# ---------- Step 7: Correct Data Types ----------\n",
    "# Convert date columns\n",
    "for col in df.columns:\n",
    "    if 'Date' in col:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"‚úÖ Data types standardized.\\n\")\n",
    "\n",
    "# ---------- Step 8: Clean Text Fields ----------\n",
    "text_cols = ['Establishment Name', 'Address', 'City', 'Infraction Details']\n",
    "for col in text_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].str.strip().str.title()\n",
    "\n",
    "print(\"‚úÖ Text fields cleaned.\\n\")\n",
    "\n",
    "# ---------- Step 9: Export Cleaned Dataset ----------\n",
    "clean_dir = \"data_cleaned\"\n",
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "clean_path = os.path.join(clean_dir, \"DineSafe_Cleaned.csv\")\n",
    "df.to_csv(clean_path, index=False)\n",
    "\n",
    "print(f\"üíæ Cleaned dataset saved to: {clean_path}\")\n",
    "print(f\"‚úÖ Final shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "#  Export as Parquet file\n",
    "parquet_path = os.path.join(clean_dir, \"DineSafe_Cleaned.parquet\")\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "print(f\"üíæ Parquet file saved to: {parquet_path}\")\n",
    "\n",
    "#  Export as Delta Lake table (optional, for reproducibility)\n",
    "delta_dir = os.path.join(clean_dir, \"DineSafe_Delta\")\n",
    "write_deltalake(delta_dir, df)\n",
    "print(f\"üíæ Delta Lake versioned dataset created at: {delta_dir}\")\n",
    "\n",
    "#  Final summary\n",
    "print(f\"‚úÖ Final shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4bf69-3db4-48ea-b68c-68c1efa23bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422b85f-bf3c-4a2b-b837-c6cfb2e72c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
